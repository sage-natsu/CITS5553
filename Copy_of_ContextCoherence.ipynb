{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from transformers import BigBirdForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import joblib\n",
        "import os\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "IlE4Aq0mts3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Class for defining the custom dataset\n",
        "class DialogueDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context = self.dataframe.iloc[idx, 0]\n",
        "        response = self.dataframe.iloc[idx, 1]\n",
        "        label = self.dataframe.iloc[idx, 2]\n",
        "\n",
        "        combined_text = context + \" \" + self.tokenizer.sep_token + \" \" + response\n",
        "        encoding = self.tokenizer(\n",
        "            combined_text,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        input_ids = encoding[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = encoding[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Step 3: Class for model training\n",
        "class ModelTrainer:\n",
        "    def __init__(self, train_dataset):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
        "        self.model = BigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base')\n",
        "        self.train_dataset = train_dataset\n",
        "        self.training_args = self._setup_training_args()\n",
        "        for name, param in self.model.named_parameters():\n",
        "          if not param.is_contiguous():\n",
        "            print(f'Making contiguous:{name}')\n",
        "            param.data = param.data.contiguous()\n",
        "        for name, param in self.model.named_parameters():\n",
        "            print(f'Layer:{name}, Contiguous:{param.is_contiguous()}')\n",
        "\n",
        "    def _setup_training_args(self):\n",
        "        # Set up training arguments, limiting to 1 epoch for quick testing\n",
        "        return TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            num_train_epochs=1,  # Quick testing with 1 epoch\n",
        "            per_device_train_batch_size=8,\n",
        "            learning_rate=2e-5,\n",
        "            warmup_steps=500,\n",
        "            weight_decay=0.01,\n",
        "            logging_dir='./logs',\n",
        "            logging_steps=50,\n",
        "            save_total_limit=2,\n",
        "            save_steps=200,\n",
        "            evaluation_strategy=\"no\",\n",
        "        )\n",
        "\n",
        "    def fine_tune_model(self):\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=self.training_args,\n",
        "            train_dataset=self.train_dataset\n",
        "        )\n",
        "        trainer.train()\n",
        "        return self.model\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        self.model.save_pretrained(save_path)\n",
        "        self.tokenizer.save_pretrained(save_path)\n",
        "        print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Step 4: Class for coherence evaluation\n",
        "class CoherenceEvaluator:\n",
        "    def __init__(self, model_path):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = BigBirdForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "    def tokenize_input(self, context, response):\n",
        "        return self.tokenizer(context, response, return_tensors='pt', max_length=1024, truncation=True, padding='max_length')\n",
        "\n",
        "    def compute_logits(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs.logits\n",
        "\n",
        "    def apply_softmax(self, logits):\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        return probabilities[0][1].item()\n",
        "\n",
        "# Step 5: Main pipeline class to encapsulate the entire process\n",
        "class CoherencePipeline:\n",
        "    def __init__(self, dataset_path, model_save_path, train_model=True):\n",
        "        self.file_path = dataset_path\n",
        "        self.model_save_path = model_save_path\n",
        "        self.train_model = train_model\n",
        "        self.model_trainer = None\n",
        "        self.coherence_evaluator = None\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "        df = pd.read_csv(self.file_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained('google/bigbird-roberta-base')\n",
        "        train_dataset = DialogueDataset(df, tokenizer, max_length=256)\n",
        "        return train_dataset\n",
        "\n",
        "    def train_and_save_model(self, train_dataset):\n",
        "        self.model_trainer = ModelTrainer(train_dataset)\n",
        "        trained_model = self.model_trainer.fine_tune_model()\n",
        "        self.model_trainer.save_model(self.model_save_path)\n",
        "        return trained_model\n",
        "\n",
        "    def evaluate_coherence(self):\n",
        "        #file_name = list(self.file_path.keys())[0]\n",
        "        with open(self.file_path, 'r') as file:\n",
        "            dialogue = file.readlines()\n",
        "\n",
        "        self.coherence_evaluator = CoherenceEvaluator(self.model_save_path)\n",
        "        pairs = [(dialogue[i].strip(), dialogue[i + 1].strip()) for i in range(len(dialogue) - 1)]\n",
        "\n",
        "        scores = []\n",
        "        for context, response in pairs:\n",
        "            inputs = self.coherence_evaluator.tokenize_input(context, response)\n",
        "            logits = self.coherence_evaluator.compute_logits(inputs)\n",
        "            score = self.coherence_evaluator.apply_softmax(logits)\n",
        "            scores.append(score)\n",
        "\n",
        "        # Create DataFrame to store results\n",
        "        df_results = pd.DataFrame({\n",
        "            'Pair Number': [f'Pair {i+1}' for i in range(len(pairs))],\n",
        "            'Context': [pair[0] for pair in pairs],\n",
        "            'Response': [pair[1] for pair in pairs],\n",
        "            'Coherence Score': scores\n",
        "        })\n",
        "\n",
        "        # Calculate overall coherence score\n",
        "        overall_score = sum(scores) / len(scores)\n",
        "        df_results.loc['Overall'] = ['', '', 'Overall Coherence Score', overall_score]\n",
        "\n",
        "        return df_results\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        if self.train_model:\n",
        "            # Train model if flag is set to True\n",
        "            train_dataset = self.prepare_dataset()\n",
        "            self.train_and_save_model(train_dataset)\n",
        "        else:\n",
        "            # Check if fine-tuned model exists\n",
        "            if self.model_save_path.startswith('google/'):\n",
        "              print(f'Using pretrained model from Hugging Face:{self.model_save_path}')\n",
        "            else:\n",
        "                if not os.path.exists(self.model_save_path):\n",
        "                    raise FileNotFoundError(f\"No fine-tuned model found at {self.model_save_path}. Please train the model first.\")\n",
        "                print(f\"Using existing model from {self.model_save_path}\")\n",
        "\n",
        "        # Proceed to evaluate test data\n",
        "        df_results = self.evaluate_coherence()\n",
        "        print(df_results)\n",
        "        return df_results\n"
      ],
      "metadata": {
        "id": "qaep9R0UNpKA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Run the pipeline\n",
        "pipeline = CoherencePipeline(\n",
        "    dataset_path = '/content/dialogues_dataset.csv',\n",
        "    model_save_path='./coherence_model',\n",
        "    train_model=True  # Set to True if you want to train, False to use existing model\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "df_results = pipeline.run_pipeline()"
      ],
      "metadata": {
        "id": "eJJiQGRiaBJM",
        "outputId": "dc3b3219-fa8f-468c-dcf8-eb36dae880cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making contiguous:bert.encoder.layer.0.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.0.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.0.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.0.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.0.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.0.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.1.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.1.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.1.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.1.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.1.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.1.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.2.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.2.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.2.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.2.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.2.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.2.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.3.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.3.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.3.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.3.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.3.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.3.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.4.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.4.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.4.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.4.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.4.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.4.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.5.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.5.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.5.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.5.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.5.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.5.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.6.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.6.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.6.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.6.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.6.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.6.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.7.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.7.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.7.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.7.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.7.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.7.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.8.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.8.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.8.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.8.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.8.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.8.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.9.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.9.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.9.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.9.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.9.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.9.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.10.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.10.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.10.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.10.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.10.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.10.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.11.attention.self.query.weight\n",
            "Making contiguous:bert.encoder.layer.11.attention.self.key.weight\n",
            "Making contiguous:bert.encoder.layer.11.attention.self.value.weight\n",
            "Making contiguous:bert.encoder.layer.11.attention.output.dense.weight\n",
            "Making contiguous:bert.encoder.layer.11.intermediate.dense.weight\n",
            "Making contiguous:bert.encoder.layer.11.output.dense.weight\n",
            "Making contiguous:bert.pooler.weight\n",
            "Layer:bert.embeddings.word_embeddings.weight, Contiguous:True\n",
            "Layer:bert.embeddings.position_embeddings.weight, Contiguous:True\n",
            "Layer:bert.embeddings.token_type_embeddings.weight, Contiguous:True\n",
            "Layer:bert.embeddings.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.embeddings.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.0.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.1.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.2.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.3.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.4.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.5.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.6.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.7.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.8.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.9.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.10.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.self.query.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.self.query.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.self.key.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.self.key.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.self.value.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.self.value.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.attention.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.intermediate.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.intermediate.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.output.dense.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.output.dense.bias, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.output.LayerNorm.weight, Contiguous:True\n",
            "Layer:bert.encoder.layer.11.output.LayerNorm.bias, Contiguous:True\n",
            "Layer:bert.pooler.weight, Contiguous:True\n",
            "Layer:bert.pooler.bias, Contiguous:True\n",
            "Layer:classifier.dense.weight, Contiguous:True\n",
            "Layer:classifier.dense.bias, Contiguous:True\n",
            "Layer:classifier.out_proj.weight, Contiguous:True\n",
            "Layer:classifier.out_proj.bias, Contiguous:True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 256 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [102/102 38:31, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.502200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./coherence_model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-442e7a97ba35>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Run the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-d429dcdd0d93>\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# Proceed to evaluate test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d429dcdd0d93>\u001b[0m in \u001b[0;36mevaluate_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoherence_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoherence_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoherence_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-d429dcdd0d93>\u001b[0m in \u001b[0;36mcompute_logits\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2740\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2742\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   2743\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2744\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2125\u001b[0m         )\n\u001b[1;32m   2126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2127\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   2128\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, band_mask, from_mask, to_mask, blocked_encoder_mask, return_dict)\u001b[0m\n\u001b[1;32m   1620\u001b[0m                 )\n\u001b[1;32m   1621\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1623\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1543\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1427\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Run the pipeline (using trained model)\n",
        "pipeline = CoherencePipeline(\n",
        "    dataset_path = '/content/dialogue1.txt',\n",
        "    model_save_path='./coherence_model',\n",
        "    train_model=False  # Set to True if you want to train, False to use existing model\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "df_result = pipeline.run_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzqZsjJT6hyT",
        "outputId": "06a1add7-63d1-4498-b958-da823b9475e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing model from ./coherence_model\n",
            "        Pair Number                                            Context  \\\n",
            "0            Pair 1  [\"AI: Hi, my name is Lila. I'm Octivo's AI age...   \n",
            "1            Pair 2  \"Caller: Hey, nice to meet you. My name is Mic...   \n",
            "2            Pair 3  \"AI: Thank you for introducing yourself Michae...   \n",
            "3            Pair 4  \"Caller: Yeah, sure. I'm 27 but I feel like I ...   \n",
            "4            Pair 5  \"AI: I completely understand your hesitation a...   \n",
            "5            Pair 6  \"Caller: Ok, that's fair enough. So I'm earnin...   \n",
            "6            Pair 7  \"AI: Thank you for sharing your income range t...   \n",
            "7            Pair 8  \"Caller: I will retire at around 65 and I woul...   \n",
            "Overall                                                                  \n",
            "\n",
            "                                                  Response  Coherence Score  \n",
            "0        \"Caller: Hey, nice to meet you. My name is Mic...         0.590054  \n",
            "1        \"AI: Thank you for introducing yourself Michae...         0.766010  \n",
            "2        \"Caller: Yeah, sure. I'm 27 but I feel like I ...         0.754133  \n",
            "3        \"AI: I completely understand your hesitation a...         0.769230  \n",
            "4        \"Caller: Ok, that's fair enough. So I'm earnin...         0.767711  \n",
            "5        \"AI: Thank you for sharing your income range t...         0.761523  \n",
            "6        \"Caller: I will retire at around 65 and I woul...         0.762877  \n",
            "7        \"AI: It was an absolute pleasure assisting you...         0.767307  \n",
            "Overall                            Overall Coherence Score         0.742356  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Run the pipeline (using bigbird pretrained model)\n",
        "pipeline = CoherencePipeline(\n",
        "    dataset_path = '/content/dialogue1.txt',\n",
        "    model_save_path='.google/bigbird-roberta-base',\n",
        "    train_model=False  # Set to True if you want to train, False to use existing model\n",
        ")\n",
        "\n",
        "# Run the pipeline\n",
        "df_results = pipeline.run_pipeline()"
      ],
      "metadata": {
        "id": "deASXtjQd50-",
        "outputId": "3e6ec584-b369-4cad-84a6-035e89422fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pretrained model from Hugging Face:google/bigbird-roberta-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Pair Number                                            Context  \\\n",
            "0            Pair 1  [\"AI: Hi, my name is Lila. I'm Octivo's AI age...   \n",
            "1            Pair 2  \"Caller: Hey, nice to meet you. My name is Mic...   \n",
            "2            Pair 3  \"AI: Thank you for introducing yourself Michae...   \n",
            "3            Pair 4  \"Caller: Yeah, sure. I'm 27 but I feel like I ...   \n",
            "4            Pair 5  \"AI: I completely understand your hesitation a...   \n",
            "5            Pair 6  \"Caller: Ok, that's fair enough. So I'm earnin...   \n",
            "6            Pair 7  \"AI: Thank you for sharing your income range t...   \n",
            "7            Pair 8  \"Caller: I will retire at around 65 and I woul...   \n",
            "Overall                                                                  \n",
            "\n",
            "                                                  Response  Coherence Score  \n",
            "0        \"Caller: Hey, nice to meet you. My name is Mic...         0.520310  \n",
            "1        \"AI: Thank you for introducing yourself Michae...         0.543495  \n",
            "2        \"Caller: Yeah, sure. I'm 27 but I feel like I ...         0.541085  \n",
            "3        \"AI: I completely understand your hesitation a...         0.538560  \n",
            "4        \"Caller: Ok, that's fair enough. So I'm earnin...         0.535641  \n",
            "5        \"AI: Thank you for sharing your income range t...         0.541511  \n",
            "6        \"Caller: I will retire at around 65 and I woul...         0.536740  \n",
            "7        \"AI: It was an absolute pleasure assisting you...         0.541023  \n",
            "Overall                            Overall Coherence Score         0.537296  \n",
            "        Pair Number                                            Context  \\\n",
            "0            Pair 1  [\"AI: Hi, my name is Lila. I'm Octivo's AI age...   \n",
            "1            Pair 2  \"Caller: Hey, nice to meet you. My name is Mic...   \n",
            "2            Pair 3  \"AI: Thank you for introducing yourself Michae...   \n",
            "3            Pair 4  \"Caller: Yeah, sure. I'm 27 but I feel like I ...   \n",
            "4            Pair 5  \"AI: I completely understand your hesitation a...   \n",
            "5            Pair 6  \"Caller: Ok, that's fair enough. So I'm earnin...   \n",
            "6            Pair 7  \"AI: Thank you for sharing your income range t...   \n",
            "7            Pair 8  \"Caller: I will retire at around 65 and I woul...   \n",
            "Overall                                                                  \n",
            "\n",
            "                                                  Response  Coherence Score  \n",
            "0        \"Caller: Hey, nice to meet you. My name is Mic...         0.520310  \n",
            "1        \"AI: Thank you for introducing yourself Michae...         0.543495  \n",
            "2        \"Caller: Yeah, sure. I'm 27 but I feel like I ...         0.541085  \n",
            "3        \"AI: I completely understand your hesitation a...         0.538560  \n",
            "4        \"Caller: Ok, that's fair enough. So I'm earnin...         0.535641  \n",
            "5        \"AI: Thank you for sharing your income range t...         0.541511  \n",
            "6        \"Caller: I will retire at around 65 and I woul...         0.536740  \n",
            "7        \"AI: It was an absolute pleasure assisting you...         0.541023  \n",
            "Overall                            Overall Coherence Score         0.537296  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}